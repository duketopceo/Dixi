# Backend Configuration
NODE_ENV=development
PORT=3001
FRONTEND_URL=http://localhost:3000
WS_PORT=3002

# Vision Service
VISION_SERVICE_URL=http://localhost:5000
VISION_SERVICE_PORT=5000
BACKEND_URL=http://localhost:3001

# Ollama Configuration (REQUIRED)
# Ollama must be installed and running before starting services
# Install: https://ollama.ai
# Start: ollama serve
# Pull model: ollama pull llama3.2
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Legacy/Unused Variables (kept for reference, not actively used)
# MODEL_PATH=./models
# MODEL_SIZE=7B
# USE_GPU=true
# GPU_DEVICE=cuda:0
# RENDER_API=webgl
# ENABLE_GPU_ACCELERATION=true
