# Cursor AI Copilot Instructions for Dixi Project

## Project Overview
Dixi is an AI-powered interactive projection system using:
- **Frontend**: React 18 + TypeScript + Vite + Three.js
- **Backend**: Node.js + Express + TypeScript + WebSocket
- **AI Service**: Ollama (not TensorFlow.js)
- **Vision**: Python + Flask + MediaPipe (optional)

## Critical Architecture Notes

### AI Service Integration
- **ALWAYS use Ollama**, never TensorFlow.js
- Ollama base URL: `http://localhost:11434` (configurable via `OLLAMA_BASE_URL`)
- Default model: `gemma3:4b` (configurable via `OLLAMA_MODEL`)
- AI service file: `packages/backend/src/services/ai.ts`
- Makes HTTP requests to Ollama's `/api/generate` endpoint
- Supports both streaming and non-streaming inference

### Frontend Configuration
- Vite dev server: Port 3000 (may fallback to 5173 if 3000 is busy)
- API proxy: `/api/*` → `http://localhost:3001`
- WebSocket: `ws://localhost:3002`
- Environment variables use `VITE_*` prefix
- TypeScript types for `import.meta.env` in `src/vite-env.d.ts`

### Backend Configuration
- HTTP server: Port 3001
- WebSocket server: Port 3002
- Health checks verify Ollama connection (not GPU)
- All TensorFlow.js references removed

### Port Assignments
- Frontend: 3000 (or 5173 if 3000 unavailable)
- Backend API: 3001
- WebSocket: 3002
- Vision Service: 5000 (optional)
- Ollama: 11434

## Code Style Guidelines

### TypeScript
- Use strict mode
- Fix all TypeScript errors before committing
- Use proper types for `import.meta.env` via vite-env.d.ts
- Remove unused imports (enforced by tsconfig)

### Error Handling
- Always handle Ollama connection failures gracefully
- Log errors using the logger utility
- Provide user-friendly error messages
- All API calls must have try-catch blocks with proper error handling
- WebSocket operations must handle connection failures and retries
- Port conflicts should be detected and provide helpful resolution instructions
- Never let errors crash the application - always handle gracefully

### Testing
- Run `scripts/test_everything.ps1` before major commits
- Test both port 3000 and 5173 for frontend
- Verify Ollama connection in health checks

## Common Tasks

### Adding New API Endpoints
1. Add route in `packages/backend/src/routes/`
2. Add service method if needed
3. Update frontend `packages/frontend/src/services/api.ts`
4. Add TypeScript types

### Updating AI Service
- Always test Ollama connection first
- Handle streaming vs non-streaming properly
- Include gesture context in prompts when available
- Set appropriate timeouts (60s for regular, 120s for streaming)
- Streaming endpoint must wait for stream completion before resolving Promise
- All errors must be caught and handled gracefully

### Frontend Development
- Use environment variables for URLs (VITE_* prefix)
- Test on both localhost and 127.0.0.1 if issues occur
- Check vite.config.ts for proxy settings
- Ensure vite-env.d.ts exists for TypeScript

## Files to Update When Making Changes

### AI_to_Khan.md
- Update this file with any significant changes
- Keep it in natural, conversational language
- Include technical details but make it readable

### Test Script
- Update `scripts/test_everything.ps1` if adding new services
- Test all critical endpoints
- Include both port 3000 and 5173 for frontend

## Git Workflow
- Commit messages should be descriptive
- Push to current branch (usually `local-cloud-exec-eedf3`)
- Update AI_to_Khan.md before pushing major changes

## Troubleshooting

### Port Conflicts
- If you get "EADDRINUSE" errors, use `npm run kill-port <port>` to kill the process
- Or manually: `Get-NetTCPConnection -LocalPort <port> | Select-Object -ExpandProperty OwningProcess | ForEach-Object { Stop-Process -Id $_ -Force }`
- Backend now provides helpful error messages with instructions when ports are in use

### Frontend Not Loading
- Check if port 3000 is in use (may fallback to 5173)
- Verify vite.config.ts has `host: '0.0.0.0'` for network binding
- Test both `localhost` and `127.0.0.1`

### Ollama Connection Issues
- Verify Ollama is running: `curl http://localhost:11434/api/tags`
- Check `OLLAMA_BASE_URL` environment variable
- Ensure model is available: `ollama list`

### TypeScript Errors
- Ensure `vite-env.d.ts` exists and is included in tsconfig.json
- Remove unused imports
- Fix type mismatches immediately

### WebSocket Connection Issues
- Frontend automatically retries up to 5 times with exponential backoff
- Check backend WebSocket server is running on port 3002
- Verify firewall isn't blocking connections

### Streaming Endpoint Issues
- Streaming endpoint now properly waits for stream completion
- Race condition fixed - all data is sent before response ends
- Errors are sent as SSE events if headers are already set

## Never Do
- ❌ Use TensorFlow.js (removed, use Ollama)
- ❌ Hardcode ports (use environment variables)
- ❌ Skip TypeScript error fixes
- ❌ Commit without testing
- ❌ Remove Ollama integration
- ❌ Use fake/placeholder data (see .github/copilot-instructions.md)
- ❌ Skip error handling in API calls or WebSocket operations
- ❌ Let errors crash the application without graceful handling

